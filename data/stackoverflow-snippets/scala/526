scala> val df = spark.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").option("basePath", "/Users/igork/testdata/").load("/Users/igork/testdata/*")
df: org.apache.spark.sql.DataFrame = [A: int, B: int ... 2 more fields]

scala> df.printSchema()
root
 |-- A: integer (nullable = true)
 |-- B: integer (nullable = true)
 |-- C: integer (nullable = true)
 |-- date: string (nullable = true)


scala> val rdd = df.filter($"date" >= "2018-01-01" && $"date" <= "2018-01-02").rdd
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[25] at rdd at <console>:25

scala> rdd.collect().foreach(println)
[11,12,13,2018-01-02]
[14,15,16,2018-01-02]
[1,2,3,2018-01-01]
[4,5,6,2018-01-01]
