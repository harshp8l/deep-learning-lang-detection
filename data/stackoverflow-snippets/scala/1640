df.show

// +---+---+---------------+
// | c1| c2|             c3|
// +---+---+---------------+
// |  1|  1|[1.0, 1.0, 3.4]|
// |  1|  2|[1.0, 0.0, 4.3]|
// |  2|  1|[0.0, 0.0, 0.0]|
// |  2|  2|[1.2, 1.1, 1.1]|
// +---+---+---------------+

import org.apache.spark.sql.expressions.Window

val window = Window.partitionBy($"c1", $"c2").orderBy($"c1", $"c2")

df.withColumn("c3", explode($"c3") )
  .withColumn("rn", row_number() over window)
  .groupBy($"c1", $"rn").agg(sum($"c3").as("c3") )
  .orderBy($"c1", $"rn")
  .groupBy($"c1")
  .agg(collect_list($"c3").as("c3prime") ).show

// +---+--------------------+
// | c1|             c3prime|
// +---+--------------------+
// |  1|[2.0, 1.0, 7.6999...|
// |  2|     [1.2, 1.1, 1.1]|
// +---+--------------------+
