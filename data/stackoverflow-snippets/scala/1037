scala> val sourceList=Array(""a"",""b"",""c"")
sourceList: Array[String] = Array(a, b, c)

scala> val grfRDD = sc.parallelize(sourceList)
grfRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:29

val grfDF = grfRDD.toDF()
grfDF: org.apache.spark.sql.DataFrame = [_1: string]

scala> grfDF
res0: org.apache.spark.sql.DataFrame = [_1: string]

scala> val row1 = grfDF.limit(1)
row1: org.apache.spark.sql.DataFrame = [_1: string]

scala> row1
res1: org.apache.spark.sql.DataFrame = [_1: string]

row1.collect()
res2: Array[org.apache.spark.sql.Row] = Array([a])

scala> val tail = grfDF.except(row1)
tail: org.apache.spark.sql.DataFrame = [_1: string]

scala> tail.collect()
res6: Array[org.apache.spark.sql.Row] = Array([b], [c])
