val columns = df.columns.tail   //selecting columns to be changed to rows

import org.apache.spark.sql.functions._
//defining udf for zipping the column names with value and returning as array of column names zipped with column values
def zipUdf = udf((cols: collection.mutable.WrappedArray[String], vals: collection.mutable.WrappedArray[String]) => cols.zip(vals))

df.select(col(""ak""), zipUdf(lit(columns), array(columns.map(col): _*)).as(""depth""))   //calling udf function above
    .withColumn(""depth"", explode(col(""depth"")))                                       //exploding the array column to be on separate rows
    .select(col(""ak""), col(""depth._1"").as(""depth""), col(""depth._2"").as(""user_count"")) //selecting columns as required in output
  .show(false)
