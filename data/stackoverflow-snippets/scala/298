import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

val df=Seq((""abcde"",""vfc""),(""abcde"",""vfc""),(""abcde"",""vfc""),(""abcde"",""dfj""),
      (""abcde"",""dfj""),(""abcde"",""wek""),(""dghkl"",""tyu""),(""dghkl"",""tyu""),(""dghkl"",""tyu"")).toDF(""col1"",""col2"")
    val WinSpec=Window.partitionBy('col1).orderBy('col2)
    df.withColumn(""col3"",dense_rank().over(WinSpec))
      .withColumn(""col3"",when('col3===1,0).otherwise('col3-1)).orderBy('col1).show(false)


+-----+----+----+
|col1 |col2|col3|
+-----+----+----+
|abcde|dfj |0   |
|abcde|dfj |0   |
|abcde|vfc |1   |
|abcde|vfc |1   |
|abcde|vfc |1   |
|abcde|wek |2   |
|dghkl|tyu |0   |
|dghkl|tyu |0   |
|dghkl|tyu |0   |
+-----+----+----+
