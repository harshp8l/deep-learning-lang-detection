// create input rdd 
val rdd1 = spark.sparkContext.makeRDD(Array((""EMP1"",0),(""EMP2"",1),(""EMP3"",2),(""EMP4"",3),(""EMP5"",4),(""EMP6"",5),(""EMP7"",6),(""EMP8"",7)))
val rdd2 = spark.sparkContext.makeRDD(Array((0,3,""ABC""),(3,5,""XYZ""),(5,1000,""PQR"")))

// 1. perform a cross join/cartesian, the rdd looks like ((""EMP1"",0), (0,3,""ABC""))
// 2. filter out those records which are not within range
// 3. formatting 
rdd1.cartesian(rdd2)
    .filter(record => record._1._2 >= record._2._1 && record._1._2 < record._2._2)
    .map(record => (record._1._1 + "" "" + record._2._3, record._1._2))
    .collect().foreach(println(_))

// result
(EMP1 ABC,0)
(EMP2 ABC,1)
(EMP3 ABC,2)
(EMP4 XYZ,3)
(EMP5 XYZ,4)
(EMP6 PQR,5)
(EMP7 PQR,6)
(EMP8 PQR,7)
