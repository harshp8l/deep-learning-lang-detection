from pyspark.sql import SQLContext

from pyspark.sql.functions import concat_ws

d1=sc.parallelize([(1, "a"), (2, "a"), (3,"a")]).toDF().toDF("Col_A","Col_B")

d2=sc.parallelize([(1, "b"), (2, "b")]).toDF().toDF("Col_A", "Col_B")

d3=sc.parallelize([(2, "c"), (3, "c")]).toDF().toDF("Col_A", "Col_B")

d4=d1.join(d2,'Col_A','left').join(d3,'Col_A','left').select(d1.Col_A.alias("col A"),concat_ws(',',d1.Col_B,d2.Col_B,d3.Col_B).alias("col B"))

df4.show()

+-----+-----+

|col A|col B|

+-----+-----+

|    1
|  a,b|

|    2
|a,b,c|

|    3
|  a,c|

+-----+-----+
