import org.apache.spark.sql.expressions.Window
def windowSpec = Window.partitionBy(""fi_Sk"", ""sec_SK"")

import org.apache.spark.sql.functions._
df.withColumn(""Visits"", count(""fi_Sk"").over(windowSpec))
//      .sort(""fi_Sk"", ""END_DATE"")
//      .show(false)
//
//    +-----+------+--------+------+
//    |fi_Sk|sec_SK|END_DATE|Visits|
//    +-----+------+--------+------+
//    |51   |42    |20130616|2     |
//    |51   |42    |20140116|2     |
//    |89   |44    |20100608|1     |
//    |89   |42    |20150330|2     |
//    |89   |42    |20160122|2     |
//    +-----+------+--------+------+
