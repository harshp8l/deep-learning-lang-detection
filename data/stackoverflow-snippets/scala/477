  import org.apache.spark.sql.functions._

  val d1 = spark.sparkContext.parallelize(Seq(
    (""A"", ""U"", ""PIERRE"", 1),
    (""A"", ""U"", ""THOMAS"", 2),
    (""A"", ""U"", ""MICHAEL"", 3),
    (""A"", ""V"", ""TOM"", 2),
    (""A"", ""V"", ""JACK"", 3),
    (""A"", ""W"", ""MICHEL"", 2),
    (""A"", ""W"", ""JULIEN"", 3)
  )).toDF(""ID1"", ""ID2"", ""VAL1"", ""VAL2"")


  d1.groupBy(""ID1"", ""ID2"").agg(first(struct(""VAL1"", ""VAL2"")).as(""val""))
    .select(""ID1"", ""ID2"", ""val.*"")
    .show(false)
