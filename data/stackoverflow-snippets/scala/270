val df1 = Seq(
  (1, "05/02/2010", 249),
  (2, "12/02/2010", 455),
  (3, "19/02/2010", 415),
  (4, "26/02/2010", 194)
).toDF("Store", "Date", "Weekly_Sales")

val df2 = Seq(
  (5, "05/02/2010", 400),
  (6, "12/02/2010", 460),
  (7, "19/02/2010", 477),
  (8, "26/02/2010", 345)
).toDF("Store", "Date", "Weekly_Sales")

import org.apache.spark.sql.expressions.Window

val window = Window.partitionBy($"Date")

df1.union(df2).
  withColumn("Avg_Sales", avg($"Weekly_Sales").over(window)).
  withColumn("Store_List", collect_list($"Store").over(window)).
  orderBy($"Date", $"Store").
  show

// +-----+----------+------------+---------+----------+
// |Store|      Date|Weekly_Sales|Avg_Sales|Store_List|
// +-----+----------+------------+---------+----------+
// |    1|05/02/2010|         249|    324.5|    [1, 5]|
// |    5|05/02/2010|         400|    324.5|    [1, 5]|
// |    2|12/02/2010|         455|    457.5|    [2, 6]|
// |    6|12/02/2010|         460|    457.5|    [2, 6]|
// |    3|19/02/2010|         415|    446.0|    [3, 7]|
// |    7|19/02/2010|         477|    446.0|    [3, 7]|
// |    4|26/02/2010|         194|    269.5|    [4, 8]|
// |    8|26/02/2010|         345|    269.5|    [4, 8]|
// +-----+----------+------------+---------+----------+
