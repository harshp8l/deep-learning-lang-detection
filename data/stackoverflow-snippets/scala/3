import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.sum

val w = Window.partitionBy(""id"")

df2.withColumn(
  ""other_occurences_sum"", sum($""occurences"").over(w) - $""occurences""
).show
// +---+----+----------+--------------------+     
// | id|item|occurences|other_occurences_sum|
// +---+----+----------+--------------------+
// |  0|car3|         2|                   3|
// |  0|car2|         1|                   4|
// |  0|car1|         2|                   3|
// |  1|car2|         1|                   1|
// |  1|car1|         1|                   1|
// +---+----+----------+--------------------+
