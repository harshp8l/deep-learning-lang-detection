scala> case class Foo(a:String, b:String, c:String)
defined class Foo

scala> val ds = spark.createDataset(List(Foo(""1"",""1"",null), Foo(""1"",null,null), Foo(""1"",""3"",null), Foo(""1"", null, null)))
ds: org.apache.spark.sql.Dataset[Foo] = [a: string, b: string ... 1 more field]

scala> val collected = ds.groupBy(ds(""a"")).agg(collect_list(ds(""b"")).alias(""b""), collect_list(ds(""c"")).alias(""c""))
collected: org.apache.spark.sql.DataFrame = [a: string, b: array<string> ... 1 more field]

scala> val filtered = collected.where(size(collected(""b"")) > 0 and size(collected(""c"")) > 0)
filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [a: string, b: array<string> ... 1 more field]

scala> collected.show
+---+------+---+
|  a|     b|  c|
+---+------+---+
|  1|[1, 3]| []|
+---+------+---+


scala> filtered.show
+---+---+---+
|  a|  b|  c|
+---+---+---+
+---+---+---+
