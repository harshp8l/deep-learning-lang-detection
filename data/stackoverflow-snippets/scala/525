  val df = spark
    .read
    .format("com.databricks.spark.csv")
    .option("header", "true") // Use first line of all files as header
    .option("inferSchema", "true") // Automatically infer data types
    .option("basePath", "hdfs://user/cloudera/") // see https://spark.apache.org/docs/latest/sql-programming-guide.html#partition-discovery
    .load("hdfs://user/cloudera/*")

  val rdd = df
    .filter($"date" >= "2018-01-01" && $"date" <= "2018-01-31")
    .rdd
