import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}

val schema = new StructType().add(
             StructField(""provider"", StringType, true)).add(
             StructField(""patient"", StringType, true)).add(
             StructField(""date"", StringType, true)).add(
             StructField(""x"", IntegerType, true)).add(
             StructField(""grp"", IntegerType, true))

def f(iter: Iterator[Row]) : Iterator[Row] = {
  iter.scanLeft(Row(""_"", ""_"", ""000000"", 0, 0))
  {
    case (x1, x2) =>

    val x = 
    if (x2.getString(2).replaceAll(""-"", """").substring(0, 6).toInt ==
        x1.getString(2).replaceAll(""-"", """").substring(0, 6).toInt + 1) 
    (0) else (1);

    val grp = x1.getInt(4) + x;

    Row(x2.getString(0), x2.getString(1), x2.getString(2), x, grp);
  }.drop(1)
}

val df_mod = spark.createDataFrame(my_df.repartition($""provider"", $""patient"")
                                        .sortWithinPartitions($""date"")
                                        .rdd.mapPartitions(f, true), schema)

import org.apache.spark.sql.expressions.Window
val windowSpec = Window.partitionBy($""provider"", $""patient"", $""grp"")
df_mod.withColumn(""consecutive_id"", count(lit(""1"")).over(windowSpec)
     ).orderBy($""provider"", $""patient"", $""date"").show
