//no of rows required
val rows = 15
//no of columns required
val cols = 10

val spark = SparkSession.builder
  .master(""local[*]"")
  .appName(""testApp"")
  .config(""spark.sql.warehouse.dir"", ""file:///c:/tmp/spark-warehouse"")
  .getOrCreate()

import spark.implicits._

val columns = 1 to cols map (i => ""col"" + i)

// create the DataFrame schema with these columns (in that order)
val schema = StructType(columns.map(StructField(_, IntegerType)))

val lstrows = Seq.fill(rows * cols)(Random.nextInt(100) + 1).grouped(cols).toList.map { x => Row(x: _*) }

val rdd = spark.sparkContext.makeRDD(lstrows)
val df = spark.createDataFrame(rdd, schema)
