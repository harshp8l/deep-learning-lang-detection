import org.apache.spark.sql.SQLContext

val sqlContext = new SQLContext(sc)
val df = sqlContext.read
    .format(""com.crealytics.spark.excel"")
    .option(""sheetName"", ""Daily"") // Required
    .option(""useHeader"", ""true"") // Required
    .option(""treatEmptyValuesAsNulls"", ""false"") // Optional, default: true
    .option(""inferSchema"", ""false"") // Optional, default: false
    .option(""addColorColumns"", ""true"") // Optional, default: false
    .option(""startColumn"", 0) // Optional, default: 0
    .option(""endColumn"", 99) // Optional, default: Int.MaxValue
    .option(""timestampFormat"", ""MM-dd-yyyy HH:mm:ss"") // Optional, default: yyyy-mm-dd hh:mm:ss[.fffffffff]
    .option(""maxRowsInMemory"", 20) // Optional, default None. If set, uses a streaming reader which can help with big files
    .option(""excerptSize"", 10) // Optional, default: 10. If set and if schema inferred, number of rows to infer schema from
    .schema(myCustomSchema) // Optional, default: Either inferred schema, or all columns are Strings
    .load(""Worktime.xlsx"")
