//lib dependency in build.sbt
""org.elasticsearch"" %% ""elasticsearch-spark-20"" % ""5.6.5""

//below is the connection variables required by Spark

val resources: String =
  s""${appConf.getString(""es-index"")}/${appConf.getString(""es.type"")}""
val esConfig: Map[String, String] = Map(
  ""es.index.auto.create"" -> s""${appConf.getString(""es.index.auto.create"")}"",
  ""es.nodes"" -> s""${appConf.getString(""es-nodes"")}"",
  ""es.port"" -> s""${appConf.getInt(""es.port"")}"",
  ""es.nodes.wan.only"" -> s""${appConf.getString(""es.nodes.wan.only"")}"",
  ""es.net.ssl"" -> s""${appConf.getString(""es.net.ssl"")}""
)

import org.elasticsearch.spark._
    val dstream: InputDStream[ConsumerRecord[String, String]] =
  KafkaUtils.createDirectStream[String, String](
    ssc,
    LocationStrategies.PreferConsistent,
    ConsumerStrategies.Subscribe[String, String](conn.topic,
                                                 conn.kafkaProps)
  )
dstream.foreachRDD(rdd =>
  rdd.map(_.value).saveJsonToEs(resources,esConfig))
ssc.checkpoint(""/tmp/OACSpark"")
ssc.start()
ssc.awaitTermination()
