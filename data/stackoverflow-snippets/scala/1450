import org.apache.spark.sql.expressions._                   //import Window library
def windowSpec = Window.partitionBy(""key"", ""value"")         //defining a window frame for the aggregation
import org.apache.spark.sql.functions._                     //importing inbuilt functions
df.withColumn(""count"", count(""value"").over(windowSpec))     // counting repeatition of value for each group of key, value and assigning that value to new column called as count
  .orderBy($""count"".desc)                                   // order dataframe with count in descending order
  .groupBy(""key"")                                           // group by key
  .agg(first(""value"").as(""value""))                          //taking the first row of each key with count column as the highest
