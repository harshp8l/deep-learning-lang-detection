val schema = Encoders.product[T].schema

// read the actual schema; This shouldn't be too expensive as Spark's
// laziness would avoid actually reading the entire file 
val fileSchema = spark.read
  .option(""header"", ""true"")
  .csv(""test.csv"").schema

// read the file using your own schema. You can later use this DF
val df = spark.read.schema(schema)
  .option(""header"", ""true"")
  .csv(""test.csv"")

// compare actual and expected column names:
val badColumnNames = fileSchema.fields.map(_.name)
  .zip(schema.fields.map(_.name))
  .filter { case (actual, expected) => actual != expected }

// fail if any inconsistency found:
assert(badColumnNames.isEmpty, 
  s""file schema does not match expected; Bad column names: ${badColumnNames.mkString(""; "")}"")
