scala> :paste
// Entering paste mode (ctrl-D to finish)

val splits = Range.Double(0,120,5).toArray

val bucketizer = new Bucketizer()
      .setInputCol("age")
      .setOutputCol("age_range_id")
      .setSplits(splits)

val df2 = bucketizer.transform(df)

// Exiting paste mode, now interpreting.

splits: Array[Double] = Array(0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0, 100.0, 105.0, 110.0, 115.0)
bucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_3c2040bf50c7
df2: org.apache.spark.sql.DataFrame = [id: bigint, age: int ... 1 more field]

scala> df2.groupBy("age_range_id").count().show
+------------+-----+
|age_range_id|count|
+------------+-----+
|         8.0|    2|
|         7.0|    1|
|        11.0|    3|
|        14.0|    2|
|         3.0|    2|
|         2.0|    1|
|        17.0|    1|
|        10.0|    1|
|         5.0|    3|
|        15.0|    2|
|        16.0|    1|
|        12.0|    1|
+------------+-----+
