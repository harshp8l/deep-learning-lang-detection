package com.scalaspark.stackoverflow
import org.apache.spark.sql.SparkSession

object StackOverFlow {
  def main(args: Array[String]): Unit = {

    def parser(lines : String): HandleMaxTuple = {
      val fileds = lines.split("","")
      val c1 = fileds(0).substring(1,10).toString()
      val c2 = fileds(1).toString()
      val c3 = fileds(2).replaceAll(""\\s"","""").toInt
      val c4 = fileds(3).replaceAll(""\\s"","""").toInt
      val c5 = fileds(4).replaceAll(""\\s"","""").toInt
      val c6 = fileds(5).replaceAll(""\\s"","""").toDouble
      val c7 = fileds(6).replaceAll(""\\s"","""").toDouble
      val c8 = fileds(7).replaceAll(""\\s"","""").toDouble
      val c9 = fileds(8).replaceAll(""\\s"","""").toDouble
      val c10 = fileds(9).replaceAll(""\\s"","""").toDouble
      val c11 = fileds(10).replaceAll(""\\s"","""").toDouble
      val c12 = fileds(11).replaceAll(""\\s"","""").toDouble
      val c13 = fileds(12).replaceAll(""\\s"","""").toDouble
      val c14 = fileds(13).replaceAll(""\\s"","""").toDouble
      val c15 = fileds(14).replaceAll(""\\s"","""").toDouble
      val c16 = fileds(15).replaceAll(""\\s"","""").toDouble
      val c17 = fileds(16).replaceAll(""\\s"","""").toDouble
      val c18 = fileds(17).replaceAll(""\\s"","""").toDouble
      val c19 = fileds(18).replaceAll(""\\s"","""").toDouble
      val c20 = fileds(19).replaceAll(""\\s"","""").toDouble
      val c21 = fileds(20).replaceAll(""\\s"","""").toDouble
      val c22 = fileds(21).replaceAll(""\\s"","""").toDouble
      val c23 = fileds(22).replaceAll(""\\s"","""").toDouble
      val c24 = fileds(23).replaceAll(""\\s"","""").toDouble
      val c25 = fileds(24).replaceAll(""\\s"","""").toDouble

      val handleMaxTuple : HandleMaxTuple = HandleMaxTuple(c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25)
      return handleMaxTuple 
    }
    val spark = SparkSession
                .builder()
                .appName(""number of tuples limit in RDD"")
                .master(""local[*]"")
                .getOrCreate()

    val lines = spark.sparkContext.textFile(""C:\\Users\\rajnish.kumar\\Desktop\\sampleData.txt"", 1)
    lines.foreach(println)
    val parseddata = lines.map(parser)
    parseddata.foreach(println)
  }

  case class HandleMaxTuple(col1:String, col2:String, col3: Int, col4: Int, col5: Int, col6: Double, col7: Double, col8: Double, col9: Double, col10: Double, col11: Double, col12: Double, col13: Double, col14: Double, col15: Double, col16: Double, col17: Double, col18: Double, col19: Double, col20: Double, col21: Double, col22: Double, col23: Double, col24: Double, col25:Double)
}
