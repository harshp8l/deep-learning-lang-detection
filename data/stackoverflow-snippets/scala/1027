import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

val df = Seq(
  (0, ""23-07-2017""),
  (0, ""26-07-2017""),
  (0, ""01-08-2017""),
  (0, ""25-08-2017""),
  (1, ""01-01-2016""),
  (1, ""04-01-2016""),
  (1, ""10-01-2016"")
).toDF(""Store_id"", ""Date_d_id"")

def daysDiff = udf(
  (d1: String, d2: String) => {
    import java.time.LocalDate 
    import java.time.temporal.ChronoUnit.DAYS

    DAYS.between(LocalDate.parse(d1), LocalDate.parse(d2))
  }
)

val df2 = df.
  withColumn( ""Date_ymd"",
    regexp_replace($""Date_d_id"", """"""(\d+)-(\d+)-(\d+)"""""", ""$3-$2-$1"")).
  withColumn( ""Prior_date_ymd"",
    lag(""Date_ymd"", 1).over(Window.partitionBy(""Store_id"").orderBy(""Date_ymd""))).
  withColumn( ""Days_diff"",
    when($""Prior_date_ymd"".isNotNull, daysDiff($""Prior_date_ymd"", $""Date_ymd"")).
    otherwise(0L))

df2.show
// +--------+----------+----------+--------------+---------+
// |Store_id| Date_d_id|  Date_ymd|Prior_date_ymd|Days_diff|
// +--------+----------+----------+--------------+---------+
// |       1|01-01-2016|2016-01-01|          null|        0|
// |       1|04-01-2016|2016-01-04|    2016-01-01|        3|
// |       1|10-01-2016|2016-01-10|    2016-01-04|        6|
// |       0|23-07-2017|2017-07-23|          null|        0|
// |       0|26-07-2017|2017-07-26|    2017-07-23|        3|
// |       0|01-08-2017|2017-08-01|    2017-07-26|        6|
// |       0|25-08-2017|2017-08-25|    2017-08-01|       24|
// +--------+----------+----------+--------------+---------+

val resultDF = df2.groupBy(""Store_id"").agg(avg(""Days_diff"").as(""Avg_diff""))

resultDF.show
// +--------+--------+
// |Store_id|Avg_diff|
// +--------+--------+
// |       1|     3.0|
// |       0|    8.25|
// +--------+--------+
