import org.apache.spark.sql.expressions._
def windowSpec = Window.partitionBy(""group"").orderBy(""date"")    //defining window function grouping by group and ordering by date

import org.apache.spark.sql.functions._
df.withColumn(""date"", to_utc_timestamp(col(""date""), ""Asia/Kathmandu""))     //converting the date to epoch datetime you can choose other timezone as required
  .withColumn(""nodeId_2"", lead(""nodeId"", 1).over(windowSpec))  //using window for creating pairs
    .filter(col(""nodeId_2"").isNotNull)                   //filtering out the unpaired rows
    .select(col(""group""), col(""nodeId"").as(""nodeId_1""), col(""nodeId_2""), col(""date""))  //selecting as required final dataframe
  .show(false)
