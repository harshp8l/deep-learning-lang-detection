func Crawl(url string, depth int, fetcher Fetcher) {
    fmt.Println("Hello from Crawl")
    if depth <= 0 {
        return
    }
    body, urls, err := fetcher.Fetch(url)
    if err != nil {
        fmt.Println(err)
        return
    }
    fmt.Printf("found: %s %q\n", url, body)
   // Adding waiting group to make sure go routines finishes
    wg := sync.WaitGroup{}
    wg.Add(len(urls))
    for _, u := range urls {
        fmt.Println("in loop with u = %s", u)
        go func() {
           defer wg.Done()
           Crawl(u, depth-1, fetcher)
        }()
    }
    wg.Wait()
}
