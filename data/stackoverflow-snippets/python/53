import multiprocessing as mp

if __name__ == "__main__":
    # imports not needed in worker
    import pandas as pd

def worker(url):
    import requests
    try:
        data = requests.get(url)
        interesting = do_your_scraping()
        return = format_for_pandas(interesting)
    except AnyExceptionsYouShouldCatch:
        return default_payload # whatever you want an error
                               # to look like
def main():
    import pandas
    with open("file_with_urls.txt") as urls_file:
        urls = [line.strip() for line in urls_file]
    with mp.Pool(4) as pool:
        df = pd.DataFrame(pool.map(worker, urls, chunksize=1)
