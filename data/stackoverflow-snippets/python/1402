import concurrent.futures
import urllib.request
from time import sleep
from bs4 import BeautifulSoup
import re
import requests


def scraper(url):
    list_counter = 0
    try:
        scrape = requests.get(url,
                              headers={""user-agent"": ""Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36""},
                              timeout=10)
        if scrape.status_code == 200:

            sleep(0.1)
            scrape = requests.get(""http://data.alexa.com/data?cli=10&dat=s&url="" + url,
                                  headers={""user-agent"": ""Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36""})
            html = scrape.content
            soup = BeautifulSoup(html, 'lxml')

            rank = re.findall(r'<popularity[^>]*text=""(\d+)""', str(soup))

            print(""Server Status:"", scrape.status_code, '-', u""\u2713"", '-', list_counter, '-', url, '-', ""Rank:"", rank[0])

            list_counter = list_counter + 1

        else:
            print(""Server Status:"", scrape.status_code)
            list_counter = list_counter + 1
            print(list_counter)
            pass

    except BaseException as e:
        exceptions.append(e)
        print()
        print(e)
        print()
        list_counter = list_counter + 1
        print(list_counter)
        pass

def load_url(url, timeout):
    with urllib.request.urlopen(url, timeout=timeout) as conn:
        return conn.read()
